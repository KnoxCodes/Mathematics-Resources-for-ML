# Probability & Statistics ðŸ“Š

This folder contains books and my handwritten notes for Probability and Statistics as applied to Machine Learning.  
Use the syllabus below as a study guide

---

## Detailed Syllabus

### 1. Foundations
- Sample space, events, axioms of probability  
- Set operations and combinatorics (counting, permutations, combinations)  
- Conditional probability, independence, and rules of probability  
- Bayesâ€™ theorem and examples

### 2. Random Variables & Functions
- Discrete vs continuous random variables  
- PMF, PDF, and CDF  
- Functions of random variables and change of variables  
- Joint, marginal, and conditional distributions

### 3. Important Distributions
- Discrete: Bernoulli, Binomial, Geometric, Poisson  
- Continuous: Uniform, Exponential, Gaussian (Normal)  
- Multivariate Normal and covariance matrices  
- Other: Gamma, Beta, Student-t (as needed in inference)

### 4. Expectation & Moments
- Expectation, linearity, and moment generating functions  
- Variance, covariance, correlation, and properties  
- Conditional expectation and law of total expectation

### 5. Limit Theorems & Sampling
- Law of Large Numbers (LLN)  
- Central Limit Theorem (CLT) and normal approximations  
- Sampling distributions and the bootstrap (intro)

### 6. Statistical Inference
- Point estimation: properties (bias, consistency, efficiency)  
- Maximum Likelihood Estimation (MLE) and method of moments  
- Confidence intervals and interpretation  
- Hypothesis testing: null/alternative, type I/II errors, p-values

### 7. Regression & Predictive Models
- Simple and multiple linear regression  
- Least squares derivation and matrix form  
- Model diagnostics, overfitting, and regularization basics

### 8. Bayesian Thinking (intro)
- Prior, likelihood, posterior, and conjugacy examples  
- Bayesian vs frequentist perspectives (intuition and use-cases)

### 9. Computational Methods
- Monte Carlo simulation basics and expectation estimation  
- Markov Chain Monte Carlo (conceptual intro)  
- Numerical stability, sampling, and random number generation

### 10. Advanced / Optional Topics
- Markov chains and stochastic processes (basic properties)  
- Information-theoretic measures: entropy, KL divergence, cross-entropy  
- Multivariate statistics: PCA, covariance estimation
- Measure-theoretic notes (only if you want deeper rigor)

---

## Suggested Study Order (practical for ML)
1. Foundations â†’ 2. Random variables & distributions â†’ 3. Expectation & moments  
2. Limit theorems & sampling â†’ 4. Statistical inference (MLE, CI, tests)  
3. Regression & predictive models â†’ 4. Bayesian intro â†’ 5. Monte Carlo methods  
(Interleave practice problems and coding exercises throughout.)

---

## Recommended Books (short list)
- *Introduction to Probability* (Blitzstein & Hwang) â€” clear, exercise-heavy text. [Course Page](https://web.stanford.edu/class/cs109/) 
- *Think Stats* (Allen B. Downey) â€” practical, Python-oriented approach. :contentReference[oaicite:1]{index=1}  
- *Mathematics for Machine Learning* (Deisenroth, Faisal, Ong) â€” ML-focused math foundations (includes probability-relevant chapters). :contentReference[oaicite:2]{index=2}

---

## Best Online Courses & Lectures
- **Stanford â€” CS109: Probability for Computer Scientists** â€” practical, example-driven probability course, widely recommended for ML foundations.  :contentReference[oaicite:5]{index=5}
- **Harvard â€” Stat 110: Probability (Joe Blitzstein)** â€” excellent lecture notes, problems, and video lectures. :contentReference[oaicite:3]{index=3}  
- **MIT OCW â€” Probabilistic Systems Analysis / Introduction to Probability (6.041 / 6.431 / res-6.012 materials)** â€” rigorous course materials and problem sets. :contentReference[oaicite:4]{index=4}  
- **Khan Academy â€” Statistics & Probability** â€” approachable bite-sized lessons and practice problems for fundamentals. :contentReference[oaicite:5]{index=5}

---

## Interactive & Video Resources
- **StatQuest (Josh Starmer)** â€” short, clear videos that explain intuition behind statistical methods and ML-relevant ideas. :contentReference[oaicite:6]{index=6}  
- **Seeing Theory** â€” interactive visualizations to build intuition for probability and inference. :contentReference[oaicite:7]{index=7}

---

## Hands-on Practice
- Work through problems from Blitzstein & Hwang (lots of practice problems). :contentReference[oaicite:8]{index=8}  
- Implement simulations and estimators in Python (use `numpy`, `scipy`, `pandas`); try small Monte Carlo experiments to build intuition.

---

## How to Use This Folder
1. Start with the *Foundations* topics and follow the suggested study order.  
2. Refer to the listed books for depth and worked exercises.  
3. Use the online courses/videos for alternate explanations and guided problem-solving.  
4. Practice by coding small simulations and solving problem sets.



